{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "*Deep learning, as it is primarily used, is essentially a statistical technique for classifying \n",
    "patterns, based on sample data, using neural networks with multiple layers*\n",
    "\n",
    "Source : [Deep Learning: A Critical Appraisal](https://arxiv.org/ftp/arxiv/papers/1801/1801.00631.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Read these articles for a brief history about the history of neural nets and deep learning.\n",
    "\n",
    "[part1](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)\n",
    "[part2](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning-part-2)\n",
    "[part3](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning-part-3)\n",
    "[part4](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning-part-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common use case for deep learning is supervised learning, where once performs the task of matching a predefined set of labels based on the input. Neural nets are very good at this task of mapping inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a simple neural net with a single layer of finite neurons can approximate a wide variety of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single layer Neural Net\n",
    "\n",
    "A simple neural network with one hidden layer would look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges, EdgesAndLinkedNodes\n",
    "\n",
    "num_input = 4\n",
    "num_hidden = 4\n",
    "num_output = 1\n",
    "N = num_input + num_hidden + num_output\n",
    "\n",
    "node_indices = list(range(N))\n",
    "\n",
    "plot = figure(title='Neural net with one hidden layer', x_range=(-1,5), y_range=(-1,5))\n",
    "\n",
    "# graph construction\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "\n",
    "# royal blue - #4169E1, darkcyan - #008B8B, orange - #FFA500\n",
    "colors = ['#4169e1'] * num_input + ['#008b8b'] * num_hidden + ['#ffa500'] * num_output\n",
    "graph.node_renderer.data_source.add(colors, 'color')\n",
    "graph.node_renderer.glyph = Circle(size=20, fill_color='color')\n",
    "\n",
    "input_indices = range(num_input)\n",
    "hidden_indices = range(num_input, num_input + num_hidden)\n",
    "edges_input_to_hidden = [(inp, h) for inp in input_indices for h in hidden_indices]\n",
    "\n",
    "output_indices = range(num_input + num_hidden, N)\n",
    "edges_hidden_to_output = [(h, o) for h in hidden_indices for o in output_indices]\n",
    "\n",
    "edges = edges_input_to_hidden + edges_hidden_to_output\n",
    "start = [x for x, _ in edges]\n",
    "end = [y for _, y in edges]\n",
    "\n",
    "graph.edge_renderer.data_source.data = dict(\n",
    "    start=start,\n",
    "    end=end)\n",
    "\n",
    "### start of layout code\n",
    "input_layer_y = np.arange(num_input)\n",
    "input_layer_x = np.ones(len(input_layer_y))\n",
    "hidden_layer_y = np.arange(num_hidden)\n",
    "hidden_layer_x = 2 * np.ones(len(hidden_layer_y))\n",
    "output_layer_y = (np.arange(num_output) + max(num_input, num_hidden) - 1) / 2\n",
    "output_layer_x = 3 * np.ones(len(output_layer_y))\n",
    "\n",
    "node_x = np.concatenate([input_layer_x, hidden_layer_x, output_layer_x])\n",
    "node_y = np.concatenate([input_layer_y, hidden_layer_y, output_layer_y])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(node_x, node_y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "# add selection, and hover glyphs to edge renderers\n",
    "# graph.edge_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "# graph.edge_renderer.inspection_policy = EdgesAndLinkedNodes()\n",
    "\n",
    "plot.renderers.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The blue layer is the input layer, the inputs simply pass through these nodes. The green layer is the hidden layer. The inputs after passing through the input layer are weighted by a parameter before being passed to the nodes in the hidden layer. The hidden layer aggregates them using a summation and passes the result through a nonlinear function. The process is repeated at all layers that come after the input layer.\n",
    "\n",
    "For example, in this case, the output of the top neuron in the hidden layer can be written down mathematically as\n",
    "$$\n",
    "x_{h1} = \\phi(w_1^T x + b_1) \\\\\n",
    "x \\text{ is the input vector}\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "$$\n",
    "x_{h2} = \\phi(w_2^T x + b_2) \\\\\n",
    "x_{h3} = \\phi(w_3^T x + b_3) \\\\\n",
    "x_{h4} = \\phi(w_4^T x + b_4)\n",
    "$$\n",
    "\n",
    "The outputs of the intermediate layer can be aggregated into a single vector as\n",
    "$$\n",
    "x_h = \\begin{bmatrix}\n",
    "x_{h1} \\\\\n",
    "x_{h2} \\\\\n",
    "x_{h3} \\\\\n",
    "x_{h4}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The response of the output layer can then be written as \n",
    "$$\n",
    "x_o = \\psi(w_o^T x_h + b_o)\n",
    "$$\n",
    "\n",
    "*NOTE* - The choice of activation functions can vary from layer to layer, we will cover some common activation functions later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why go deep ?\n",
    "\n",
    "Although the universal approximation theorem states that a neural net with a single hidden layer is enough, such an architecture may not be computationally tractable. For example, the number of neurons in that single layer might exceed the total number of atoms in the universe or the neural net might take until the end of the universe to converge. We overcome this limitation by going deeper.\n",
    "\n",
    "Going deep allows you to model more complexity while keeping your parameters from growing at an exponential pace.\n",
    "\n",
    "Deep learning has become computationally feasible with the advent of modern computing power.\n",
    "\n",
    "Recommended reading - [Deep Learning Review](http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Math Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Non-linearity\n",
    "\n",
    "[Wolfram Mathematica - Linear Functions](http://mathworld.wolfram.com/LinearFunction.html)\n",
    " \n",
    " A linear function is a function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ which satisfies\n",
    "\n",
    "\\begin{align*}\n",
    "f(x+y)=f(x)+f(y)\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\alpha x) &= \\alpha f(x) \\\\\n",
    "   \\forall\\  &x, y \\in \\mathbb{R}, \\alpha \\in \\mathbb{R}. \n",
    "\\end{align*}\n",
    "\n",
    "Any function that does not exhibit linearity is a non-linear function. We typically deal with continuous differentiable **non-linear** functions in deep learning.\n",
    "\n",
    "We will take a closer look at a few examples of common non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ReLU\n",
    "The simplest non-linear function is ReLU (rectified linear unit)\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) &=  \\begin{cases}\n",
    "x & x \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-10, 10, 0.001)\n",
    "\n",
    "def relu(scalar):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU)\n",
    "    \"\"\"\n",
    "    return scalar if scalar > 0 else 0\n",
    "\n",
    "y = np.array([relu(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "\n",
    "plot = figure(title='ReLU', x_range=(-10,10), y_range=(-1,10))\n",
    "plot.line(x,y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sigmoid / logistic\n",
    "\n",
    "This is one of the most used activation functions in deep learning (especially in the output layers).\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) &= \\frac{1} {1 + \\exp(-x)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.001)\n",
    "\n",
    "def sigmoid(scalar):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-scalar))\n",
    "\n",
    "y = np.array([sigmoid(i) for i in x])\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "\n",
    "plot = figure(title='Logistic', x_range=(-10,10), y_range=(-0.5,1.5))\n",
    "plot.line(x,y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Loss functions\n",
    "\n",
    "In ML problems, we evaluate the performance of our model using a [metric](http://mathworld.wolfram.com/Metric.html)/loss function. We will look at some commonly used loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Cross-entropy\n",
    "\n",
    "A function that closely approximates classification error.\n",
    "\n",
    "\\begin{align*}\n",
    "K &- \\text{number of classes} \\\\\n",
    "n &- \\text{number of data points} \\\\\n",
    "X &- \\text{matrix of data points indexed row wise} \\\\\n",
    "y_{ik} &- \\text{response of the } k^{th} \\text{output node to } i^{th} \\text{input} \\\\\n",
    "CE(X) &= - \\sum_{i=1}^{n} \\sum_{k=1}^K y_{ik} \\log f_k(x_i)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### $\\mathcal{l}2$ loss\n",
    "\n",
    "Typically used when the response variable is continuous, e.g. linear regression\n",
    "\n",
    "\\begin{align*}\n",
    "L(X) &= \\sum_{i=1}^{n} (y - f(x))^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Convex optimization\n",
    "\n",
    "The choice of loss function greatly influences our solution to the problem. Deep learning problems are posed as minimization of carefully chosen loss functions. It is very important that you pick differentiable continuous convex functions. Non-convex optimization problems are NP-hard.\n",
    "\n",
    "We tackle convex optimization problems using gradient descent (or its variants). The main idea behind gradient descent to traverse along the opposite direction of the parameter gradient until you hit a minima (which is hopefully global or good enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression using Gradient Descent\n",
    "\n",
    "We use the math we learnt so far in order to solve a linear regression problem. We do this by posing linear regression as a convex optimization problem where the objective is to minimize $l2$ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# y = 3 x + 5, a = 3, b = 5\n",
    "x = np.random.uniform(0, 100, size=100)\n",
    "noise = np.random.normal(0, 15, size=100)\n",
    "\n",
    "y = 3 * x + 5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.regplot(x,y, ax=fig.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problem formulation (contd.)\n",
    "\n",
    "We will formulate the problem as a convex optimization problem, and minimize the error using a gradient descent approach.\n",
    "\n",
    "We will try to minimize the following error/loss/objective function,\n",
    "\n",
    "\\begin{align*}\n",
    "E(a,b) &= \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n",
    "       &= \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - ax_i - b )^2 \\\\\n",
    "       &= \\frac{1}{2N} \\sum_{i=1}^{n} y_i^2 + a^2 x_i^2 + b^2 + 2ax_ib - 2ax_iy_i - 2y_ib\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient descent\n",
    "\n",
    "Next, we need to compute the partial derivative w.r.t the parameters `a` and `b`.\n",
    "\n",
    "\\begin{align*}\n",
    "G(a,b) = \\nabla E(a,b) &= \\begin{bmatrix}\n",
    "\\frac{\\delta E}{\\delta a} \\\\\n",
    "\\frac{\\delta E}{\\delta b} \n",
    "\\end{bmatrix} \\\\\n",
    " &= \\begin{bmatrix}\n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2ax_i^2 + 2bx_i - 2x_iy_i \\\\\n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2b + 2ax_i - 2y_i\n",
    " \\end{bmatrix} \\\\\n",
    " &= \\begin{bmatrix} \n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2x_i (ax_i + b - y_i) \\\\\n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2 (ax_i + b - y_i)\n",
    " \\end{bmatrix} \\\\\n",
    " &= \\begin{bmatrix} \n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2x_i (\\hat{y}_i - y_i) \\\\\n",
    " \\frac{1}{2N} \\sum_{i=1}^{n} 2 (\\hat{y}_i - y_i)\n",
    " \\end{bmatrix} \\\\\n",
    "  &= \\begin{bmatrix} \n",
    " \\frac{1}{N} \\sum_{i=1}^{n} x_i \\text{(pointwise prediction error)} \\\\\n",
    " \\frac{1}{N} \\sum_{i=1}^{n} \\text{(pointwise prediction error)}\n",
    " \\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(x, a, b):\n",
    "    \"\"\"\n",
    "    Function estimates the response y_hat given input data x, parameters a and b\n",
    "    \"\"\"\n",
    "    ### INSERT YOUR CODE HERE ####\n",
    "    raise NotImplementedError\n",
    "\n",
    "def compute_gradient(x, y, a, b):\n",
    "    \"\"\"\n",
    "    Computes gradient in the parameter space.\n",
    "    \"\"\"\n",
    "    ### INSERT YOUR CODE HERE ####\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a, b = 0.0, 0.0\n",
    "learning_rate = 0.0001\n",
    "\n",
    "def converged(grad_a, grad_b):\n",
    "    \"\"\"\n",
    "    Checks if magnitude of gradient is less than a preset value\n",
    "    \"\"\"\n",
    "    mag_grad = grad_a * grad_a + grad_b * grad_b\n",
    "    return mag_grad <= 10e-10\n",
    "\n",
    "grad_a, grad_b = np.inf, np.inf\n",
    "num_iterations = 0\n",
    "\n",
    "# normalize your inputs before you perform gradient descent\n",
    "# or good luck getting it to converge\n",
    "normalized_x = (x - np.mean(x)) / np.std(x)\n",
    "normalized_y = (y - np.mean(y)) / np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# DONT RUN THIS CELL REPEATEDLY, DESCENT WILL NOT CONVERGE WITH POOR SEEDS\n",
    "history_a = []\n",
    "history_b = []\n",
    "\n",
    "def rescale(a,b):\n",
    "    \"\"\"\n",
    "    Rescales the coefficients a and b back into the unscaled versions\n",
    "    \"\"\"\n",
    "    b = np.std(y) * (b - a * np.mean(x) / np.std(x)) + np.mean(y)\n",
    "    a = a * np.std(y) / np.std(x)\n",
    "    return a ,b\n",
    "\n",
    "while not converged(grad_a, grad_b) and num_iterations < 10e7:\n",
    "    ### INSERT YOUR CODE HERE ####\n",
    "    raise NotImplementedError\n",
    "\n",
    "# rescale params to the \"unnormalized\" version\n",
    "a, b = rescale(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The parameters for our linear model are a={}, b={}\".format(a,b))\n",
    "errors = (a * x + b) - y\n",
    "print(\"Root mean squared error (RMSE) : \", np.sqrt(np.mean(np.power(errors, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "resp_y = a * x + b\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, resp_y, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Conclusion\n",
    "\n",
    "The manual approach to computing gradients is error prone and not easy for modern neural nets. For example, the VGG16 convnet architecture has 14 million trainable parameters (these nets are better than humans at performing large scale object recognition).\n",
    "\n",
    "This is why we use tools like TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tools\n",
    "<br/>\n",
    "<center>\n",
    "<img src=Tensorflow_logo.svg height=200/>\n",
    "<br>\n",
    "<img src=keras-logo-small-2018.jpg height=200 width=100/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### TensorFlow\n",
    "\n",
    "A matrix is a 2D ordered collection.\n",
    "A tensor is a generalization of a matrix to `n` dimensions, i.e. a tensor is a `n` dimensional ordered collection.\n",
    "\n",
    "TensorFlow is an API that makes it relatively easy to manipulate tensors. TensorFlow is suited for deep learning problems since it does a lot of things for the developer automatically. Models are constructed in the form of computational graphs, automatic differentiation using a subgraph makes it easier to do things like backpropagation with very little input from the developer.\n",
    "\n",
    "TensorFlow is open-sourced by Google, the website has a lot of tutorials and documentation regarding the use of TensorFlow. For further reading and insight into the inner workings of TensorFlow, we recommend the following links.\n",
    "\n",
    "   - [Automatic differentiation](http://www.columbia.edu/~ahd2125/post/2015/12/5)\n",
    "   - [Low level intro](https://www.tensorflow.org/programmers_guide/low_level_intro)\n",
    "   - [Programmer's guide](https://www.tensorflow.org/programmers_guide)\n",
    "\n",
    "TensorFlow is aimed at building low level computational graphs (although it does have high level modules in contrib), which is why we build using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Keras\n",
    "\n",
    "From the keras [website](https://keras.io),\n",
    "\n",
    "*Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.*\n",
    "\n",
    "*Use Keras if you need a deep learning library that*\n",
    "\n",
    "   - *Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).*\n",
    "   - *Supports both convolutional networks and recurrent networks, as well as combinations of the two.*\n",
    "   - *Runs seamlessly on CPU and GPU.*\n",
    "\n",
    "Using Keras, we can prototype neural nets fast without a lot of knowledge regarding construction of computational graphs. Keras also supports other low level libraries like Theano and CNTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### TensorBoard\n",
    "\n",
    "TensorBoard is a debugging tool that goes with TensorFlow, it is useful in visualizing your results.\n",
    "We will be using it to monitor our training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iris classification\n",
    "\n",
    "Read more about the dataset on [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is a very old dataset with 150 data points.\n",
    "\n",
    "The data set consists of measurements of the three species of the iris flower(iris setosa, iris virginica and iris versicolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "print(\"Dataset has {} data points\".format(len(iris)))\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue=\"species\", size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "y = iris[\"species\"]\n",
    "\n",
    "X_mean = X.apply(np.mean, axis=0)\n",
    "X_std  = X.apply(np.std,  axis=0)\n",
    "X_norm = (X - X_mean) / X_std\n",
    "y_label = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### INSERT YOUR CODE HERE ####\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from lib.default_utils import default_callbacks\n",
    "\n",
    "K.set_learning_phase(True)  # important if you have modules like dropout in your model\n",
    "\n",
    "### INSERT YOUR CODE HERE ####\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loss, score = model.evaluate(X_norm, y_label)\n",
    "print(\"This model has an accuracy of {}\".format(score * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
