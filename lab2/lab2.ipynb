{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DevCon 2018 - Deep Learning (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Objective\n",
    "\n",
    "In this session, we are going to build a bunch of neural nets and debug them with tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has length 60000 and consists of images of size 28 by 28\n",
      "Testing set has length 10000 and consists of images of size 28 by 28\n"
     ]
    }
   ],
   "source": [
    "with np.load('../../data/lab1/mnist.npz') as f:\n",
    "    train_x, train_y = f['x_train'], f['y_train']\n",
    "    test_x, test_y = f['x_test'], f['y_test']\n",
    "\n",
    "print(\"Training set has length {0} and consists of images of size {1} by {2}\".format(*train_x.shape))\n",
    "print(\"Testing set has length {0} and consists of images of size {1} by {2}\".format(*test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our data\n",
    "\n",
    "The first dataset we will work on consists of images of handwritten digits. The task at hand is to classify the image as a digit. These are a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "samples = train_x[np.random.choice(range(len(train_x)), size=10, replace=False), :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8,4))\n",
    "for i, sample in enumerate(samples):\n",
    "    ax = figure.add_subplot(2, 5, i+1)\n",
    "    ax.imshow(sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple feed forward neural net\n",
    "\n",
    "Our first approach will be to build a simple feed forward net, with no hidden layers. This is equivalent to multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have to flatten our image data into a one dimensional vector before it can be fed into a simple feed forward neural net. The input has to be normalized before feeding into the neural net. Normalization is done by dividing the each element in the vector by the max (255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified training set has length 60000 and consists of vectors of size 784\n",
      "Modified testing set has length 10000 and consists of vectors of size 784\n",
      "Modified training labels are of size 10\n",
      "Modified testing labels are of size 10\n"
     ]
    }
   ],
   "source": [
    "num_train_samples, width, height = train_x.shape\n",
    "num_test_samples = test_x.shape[0]\n",
    "\n",
    "flat_train_x = np.reshape(train_x, (num_train_samples, width * height))\n",
    "flat_test_x  = np.reshape(test_x, (num_test_samples, width * height))\n",
    "\n",
    "dummied_train_y = np_utils.to_categorical(train_y)\n",
    "dummied_test_y = np_utils.to_categorical(test_y)\n",
    "\n",
    "print(\"Modified training set has length {0} and consists of vectors of size {1}\".format(*flat_train_x.shape))\n",
    "print(\"Modified testing set has length {0} and consists of vectors of size {1}\".format(*flat_test_x.shape))\n",
    "print(\"Modified training labels are of size {}\".format(dummied_train_y.shape[1]))\n",
    "print(\"Modified testing labels are of size {}\".format(dummied_test_y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# normalize your input vectors\n",
    "# we do 0-1 normalization by dividing by 255\n",
    "flat_train_x = flat_train_x / 255.0\n",
    "flat_test_x = flat_test_x / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "output (Dense)               (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet_1 = Sequential()\n",
    "lenet_1.add(Dense(units=10,\n",
    "                  name=\"output\",\n",
    "                  activation=\"softmax\",\n",
    "                  input_shape=(width * height,)))\n",
    "lenet_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.8478 - acc: 0.8004 - val_loss: 0.5094 - val_acc: 0.8784\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4899 - acc: 0.8743 - val_loss: 0.4180 - val_acc: 0.8942\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.4280 - acc: 0.8863 - val_loss: 0.3818 - val_acc: 0.9002\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3975 - acc: 0.8923 - val_loss: 0.3613 - val_acc: 0.9027\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3782 - acc: 0.8965 - val_loss: 0.3474 - val_acc: 0.9065\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3646 - acc: 0.8993 - val_loss: 0.3380 - val_acc: 0.9078\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3545 - acc: 0.9016 - val_loss: 0.3299 - val_acc: 0.9100\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3463 - acc: 0.9037 - val_loss: 0.3245 - val_acc: 0.9103\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3397 - acc: 0.9052 - val_loss: 0.3189 - val_acc: 0.9118\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3341 - acc: 0.9070 - val_loss: 0.3152 - val_acc: 0.9142\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras import backend as K\n",
    "from lib.default_utils import default_callbacks\n",
    "\n",
    "K.set_learning_phase(True)  # important if you have modules like dropout in your model\n",
    "\n",
    "lenet_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_1, prefix='lenet-1', batch_size=32)\n",
    "\n",
    "# start training, use tensorboard to show overfitting with more iterations\n",
    "lenet_1.fit(x=flat_train_x, \n",
    "          y=dummied_train_y, \n",
    "          batch_size=32, \n",
    "          epochs=10, \n",
    "          verbose=True, \n",
    "          callbacks=dfcb, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_1.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[TensorBoard](http://localhost:9001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Model accuracy on test dataset is 91.410\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_1.evaluate(flat_test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model with one hidden layer\n",
    "\n",
    "We add more complexity by adding one hidden layer into our network. We will compare the performance of this network with the previous model using tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 12,730\n",
      "Trainable params: 12,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lenet_2 = Sequential()\n",
    "lenet_2.add(Dense(units=16,\n",
    "                  name=\"hidden_1\",\n",
    "                  activation=\"relu\",\n",
    "                  input_shape=(width * height,)))\n",
    "\n",
    "lenet_2.add(Dense(units=10,\n",
    "                  name=\"output\",\n",
    "                  activation=\"softmax\"))\n",
    "lenet_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.9170 - acc: 0.7563 - val_loss: 0.4447 - val_acc: 0.8820\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.4206 - acc: 0.8837 - val_loss: 0.3530 - val_acc: 0.8995\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3606 - acc: 0.8978 - val_loss: 0.3221 - val_acc: 0.9079\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.3312 - acc: 0.9058 - val_loss: 0.2996 - val_acc: 0.9147\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.3111 - acc: 0.9110 - val_loss: 0.2863 - val_acc: 0.9182\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2954 - acc: 0.9154 - val_loss: 0.2730 - val_acc: 0.9255\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2827 - acc: 0.9201 - val_loss: 0.2623 - val_acc: 0.9268\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2713 - acc: 0.9227 - val_loss: 0.2571 - val_acc: 0.9265\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2614 - acc: 0.9260 - val_loss: 0.2478 - val_acc: 0.9306\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2525 - acc: 0.9290 - val_loss: 0.2414 - val_acc: 0.9333\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2448 - acc: 0.9312 - val_loss: 0.2380 - val_acc: 0.9342\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2375 - acc: 0.9329 - val_loss: 0.2304 - val_acc: 0.9360\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2312 - acc: 0.9348 - val_loss: 0.2265 - val_acc: 0.9383\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.2251 - acc: 0.9365 - val_loss: 0.2233 - val_acc: 0.9379\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2200 - acc: 0.9372 - val_loss: 0.2183 - val_acc: 0.9395\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2150 - acc: 0.9397 - val_loss: 0.2146 - val_acc: 0.9413\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2100 - acc: 0.9404 - val_loss: 0.2122 - val_acc: 0.9412\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2054 - acc: 0.9416 - val_loss: 0.2078 - val_acc: 0.9427\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2011 - acc: 0.9427 - val_loss: 0.2080 - val_acc: 0.9410\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.1969 - acc: 0.9440 - val_loss: 0.2018 - val_acc: 0.9450\n"
     ]
    }
   ],
   "source": [
    "## build lenet-2\n",
    "lenet_2.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_2, prefix='lenet-2', batch_size=32)\n",
    "\n",
    "# start training, use tensorboard to show overfitting with more iterations\n",
    "lenet_2.fit(x=flat_train_x, \n",
    "          y=dummied_train_y, \n",
    "          batch_size=32, \n",
    "          epochs=20, \n",
    "          verbose=True, \n",
    "          callbacks=dfcb, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_2.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[TensorBoard](http://localhost:9001/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 22us/step\n",
      "Model accuracy on test dataset is 94.320\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_2.evaluate(flat_test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Simple convolutional net\n",
    "\n",
    "Regular neural nets don't scale well for images. There is structure in images which we can exploit to our advantage. Convolutional neural nets exploit this structure to perform image related tasks.\n",
    "\n",
    "** Draft **\n",
    "This explains it better than I could\n",
    "https://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 22, 22, 12)        9612      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                58090     \n",
      "=================================================================\n",
      "Total params: 68,022\n",
      "Trainable params: 68,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# lenet 3\n",
    "# 2 convolutional layers - 3x3 and 5x5 patches\n",
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "lenet_3 = Sequential()\n",
    "lenet_3.add(Conv2D(filters=32,\n",
    "                  name=\"conv_1\",\n",
    "                  kernel_size=(3,3),\n",
    "                  activation=\"relu\",\n",
    "                  padding='valid',\n",
    "                  input_shape=(width, height, 1)))\n",
    "\n",
    "lenet_3.add(Conv2D(filters=12,\n",
    "                  kernel_size=(5,5),\n",
    "                  padding='valid',\n",
    "                  name=\"conv_2\",\n",
    "                  activation=\"relu\"))\n",
    "\n",
    "lenet_3.add(Flatten())\n",
    "lenet_3.add(Dense(units=10,\n",
    "                  name='output',\n",
    "                  activation='softmax'))\n",
    "lenet_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "train_x = np.expand_dims(train_x, axis=-1)\n",
    "test_x = np.expand_dims(test_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.4626 - acc: 0.8624 - val_loss: 0.2430 - val_acc: 0.9301\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 77s 2ms/step - loss: 0.1862 - acc: 0.9479 - val_loss: 0.1312 - val_acc: 0.9630\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 73s 2ms/step - loss: 0.1096 - acc: 0.9681 - val_loss: 0.1101 - val_acc: 0.9684\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.0863 - acc: 0.9743 - val_loss: 0.0898 - val_acc: 0.9732\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 80s 2ms/step - loss: 0.0733 - acc: 0.9782 - val_loss: 0.0789 - val_acc: 0.9772\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0653 - acc: 0.9803 - val_loss: 0.0711 - val_acc: 0.9784\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 170s 4ms/step - loss: 0.0580 - acc: 0.9824 - val_loss: 0.0745 - val_acc: 0.9791\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0542 - acc: 0.9835 - val_loss: 0.0707 - val_acc: 0.9795\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 199s 4ms/step - loss: 0.0494 - acc: 0.9848 - val_loss: 0.0703 - val_acc: 0.9779\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 186s 4ms/step - loss: 0.0455 - acc: 0.9860 - val_loss: 0.0672 - val_acc: 0.9815\n"
     ]
    }
   ],
   "source": [
    "## build lenet-3\n",
    "lenet_3.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_3, prefix='conv-lenet-3', batch_size=32)\n",
    "    \n",
    "# start training, use tensorboard to show overfitting with more iterations\n",
    "lenet_3.fit(x=train_x, \n",
    "            y=dummied_train_y, \n",
    "            batch_size=32, \n",
    "            epochs=10, \n",
    "            verbose=True, \n",
    "            callbacks=dfcb, \n",
    "            validation_split=0.2,\n",
    "            shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_3.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Tensorboard](http://localhost:9001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "Model accuracy on test dataset is 98.320\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test_dataset\n",
    "_, test_accuracy = lenet_3.evaluate(test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Miscellaneous stuff\n",
    "\n",
    " - Overfitting\n",
    " \n",
    " Neural networks are notoriously easy to overfit, make sure your data set is big enough for the model that you are building. Always use a large validation set. Cross-validation can be time consuming.\n",
    " \n",
    " - Learning rate decay\n",
    " \n",
    " Always normalize your data before you feed it into the model. Gradient descent can be difficult to converge/tune without normalization\n",
    "\n",
    " - optimizers\n",
    " \n",
    " There is a large variety of optimizers out there - sgd with momentum, adam, rmsprop. We recommend sticking to sgd if you want good generalization. [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/abs/1705.08292)\n",
    " \n",
    " - pooling in convolutional nets\n",
    " \n",
    " Feature pooling is a way to reduce feature size as you go deeper in the neural net.\n",
    " \n",
    " - class imbalance\n",
    " \n",
    " Make sure your classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Exercise] Add dropout to the model\n",
    "\n",
    "Dropout is a way to prevent overfitting. add more explanation, insert link to paper.\n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 22, 22, 12)        9612      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                58090     \n",
      "=================================================================\n",
      "Total params: 68,022\n",
      "Trainable params: 68,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "lenet_4 = Sequential()\n",
    "lenet_4.add(Conv2D(filters=32,\n",
    "                  name=\"conv_1\",\n",
    "                  kernel_size=(3,3),\n",
    "                  activation=\"relu\",\n",
    "                  padding='valid',\n",
    "                  input_shape=(width, height, 1)))\n",
    "\n",
    "lenet_4.add(Dropout(0.2))\n",
    "\n",
    "lenet_4.add(Conv2D(filters=12,\n",
    "                  kernel_size=(5,5),\n",
    "                  padding='valid',\n",
    "                  name=\"conv_2\",\n",
    "                  activation=\"relu\"))\n",
    "\n",
    "lenet_4.add(Dropout(0.2))\n",
    "\n",
    "lenet_4.add(Flatten())\n",
    "lenet_4.add(Dense(units=10,\n",
    "                  name='output',\n",
    "                  activation='softmax'))\n",
    "lenet_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "Warning: Could not load \"/Users/vthom15/anaconda3/envs/python-3-6-ml/lib/graphviz/libgvplugin_pango.6.dylib\" - file not found\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 182s 4ms/step - loss: 0.4776 - acc: 0.8561 - val_loss: 0.2171 - val_acc: 0.9386\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 177s 4ms/step - loss: 0.1744 - acc: 0.9485 - val_loss: 0.1298 - val_acc: 0.9621\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 190s 4ms/step - loss: 0.1187 - acc: 0.9649 - val_loss: 0.1128 - val_acc: 0.9674\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 200s 4ms/step - loss: 0.0977 - acc: 0.9710 - val_loss: 0.0995 - val_acc: 0.9695\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 189s 4ms/step - loss: 0.0877 - acc: 0.9738 - val_loss: 0.0898 - val_acc: 0.9720\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 191s 4ms/step - loss: 0.0782 - acc: 0.9761 - val_loss: 0.0813 - val_acc: 0.9748\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 189s 4ms/step - loss: 0.0712 - acc: 0.9779 - val_loss: 0.0728 - val_acc: 0.9794\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 188s 4ms/step - loss: 0.0652 - acc: 0.9794 - val_loss: 0.0791 - val_acc: 0.9770\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 186s 4ms/step - loss: 0.0603 - acc: 0.9809 - val_loss: 0.0698 - val_acc: 0.9778\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 186s 4ms/step - loss: 0.0567 - acc: 0.9826 - val_loss: 0.0692 - val_acc: 0.9793\n"
     ]
    }
   ],
   "source": [
    "lenet_4.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "logpath, dfcb = default_callbacks(lenet_4, prefix='conv-lenet-4-dropout', batch_size=32)\n",
    "    \n",
    "# start training, use tensorboard to show overfitting with more iterations\n",
    "lenet_4.fit(x=train_x, \n",
    "            y=dummied_train_y, \n",
    "            batch_size=32, \n",
    "            epochs=10, \n",
    "            verbose=True, \n",
    "            callbacks=dfcb, \n",
    "            validation_split=0.2,\n",
    "            shuffle=True)\n",
    "\n",
    "# save final weights after completion of training\n",
    "lenet_4.save_weights(os.path.join(logpath, \"model_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 9s 946us/step\n",
      "Model accuracy on test dataset is 98.230\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test_dataset\n",
    "K.set_learning_phase(False)\n",
    "_, test_accuracy = lenet_4.evaluate(test_x, dummied_test_y)\n",
    "print(\"Model accuracy on test dataset is {:.3f}\".format(test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
